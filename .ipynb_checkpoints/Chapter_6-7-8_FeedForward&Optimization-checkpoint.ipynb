{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'Data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat images as flat vector and labels as one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(dataset, labels, id):\n",
    "    image = dataset[id]\n",
    "    image = image.reshape((image_size, image_size))\n",
    "    label = labels[id]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26efbce8fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_dataset, train_labels, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(architecture, batch_size, dropout, regularization, optimizer_name, learning_rate):\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Initialized as placeholder because this will be fed with minibatch during training\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        \n",
    "        # Constants\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        Regularizations = []\n",
    "\n",
    "        # Variables\n",
    "        for layer in range(len(architecture)):\n",
    "            if layer == 0:\n",
    "                weights.append(tf.Variable(tf.truncated_normal([image_size * image_size, architecture[layer]])))\n",
    "            else :\n",
    "                weights.append(tf.Variable(tf.truncated_normal([architecture[layer-1], architecture[layer]])))\n",
    "                \n",
    "            biases.append(tf.Variable(tf.zeros([architecture[layer]])))\n",
    "            Regularizations.append(tf.nn.l2_loss(weights[layer]))\n",
    "\n",
    "        # Training Feedforward computation.\n",
    "        def model(data):\n",
    "            h = []\n",
    "            for layer in range(len(architecture)):\n",
    "                if layer == 0:\n",
    "                    h.append(tf.nn.dropout(tf.nn.relu(tf.matmul(data, weights[layer]) + biases[layer]), keep_prob=dropout))\n",
    "                elif layer == len(architecture)-1:\n",
    "                    h.append(tf.matmul(h[layer-1], weights[layer]) + biases[layer])\n",
    "                else :\n",
    "                    h.append(tf.nn.dropout(tf.nn.relu(tf.matmul(h[layer-1], weights[layer]) + biases[layer]), keep_prob=dropout))\n",
    "            \n",
    "            return h[len(architecture)-1]\n",
    "        \n",
    "        train_logits = model(tf_train_dataset)\n",
    "\n",
    "        # Loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf_train_labels, \n",
    "                logits=train_logits) + sum([regularization*_r for _r in Regularizations]))\n",
    "\n",
    "        # Optimizer.\n",
    "        if optimizer_name == \"sgd\":\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        elif optimizer_name == \"sgd_decay\":\n",
    "            global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "            learning_rate = tf.train.exponential_decay(learning_rate, global_step, 500, 0.96)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        elif optimizer_name == \"rmsprop\":\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "        elif optimizer_name == \"adam\":\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(train_logits)\n",
    "\n",
    "        valid_logits = model(tf_valid_dataset)\n",
    "        valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "        test_logits = model(tf_test_dataset)\n",
    "        test_prediction = tf.nn.softmax(test_logits)\n",
    "        \n",
    "        return tf_train_dataset, tf_train_labels, graph, optimizer, loss, train_prediction, valid_prediction, test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(batch_size, samples_id, training_size):\n",
    "    if batch_size > len(samples_id):\n",
    "        new_samples = list(range(training_size-1))\n",
    "        random.shuffle(new_samples)\n",
    "        samples_id.extend(new_samples)\n",
    "    \n",
    "    next_batch = samples_id[:batch_size]\n",
    "    samples_id = samples_id[batch_size:]\n",
    "    \n",
    "    return next_batch, samples_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_nn(architecture, batch_size, num_epochs, early_stopping_patience, dropout=1, regularization=0, optimizer_name='sgd', learning_rate=0.1):\n",
    "    nb_steps = train_dataset.shape[0] // batch_size\n",
    "    tf_train_dataset, tf_train_labels, graph, optimizer, loss, train_prediction, valid_prediction, test_prediction = neural_net(architecture,\n",
    "                                                                                                                                batch_size, \n",
    "                                                                                                                                dropout, \n",
    "                                                                                                                                regularization, \n",
    "                                                                                                                                optimizer_name,\n",
    "                                                                                                                                learning_rate)\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        early_stopping = False\n",
    "        for epoch in range(num_epochs):\n",
    "            if early_stopping : break\n",
    "            print(\"Epoch : \" + str(epoch))\n",
    "            ids = []\n",
    "            for step in range(nb_steps):\n",
    "                batch_ids, ids = get_minibatch(batch_size, ids, train_dataset.shape[0])\n",
    "                batch_data = train_dataset[batch_ids, :]\n",
    "                batch_labels = train_labels[batch_ids, :]\n",
    "\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % 500 == 0):\n",
    "                    print(\"Minibatch loss at epoch %d and step %d: %f\" % (epoch, step, l))\n",
    "                    print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                    print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "                    \n",
    "                # Early Stopping\n",
    "                min_delta = 0.01\n",
    "                \n",
    "                if step == 0:\n",
    "                    patience_cnt = 0\n",
    "                if step > 0 and old_loss - l > min_delta:\n",
    "                    patience_cnt = 0\n",
    "                else:\n",
    "                    patience_cnt += 1\n",
    "                    \n",
    "                old_loss = l\n",
    "\n",
    "                if patience_cnt > early_stopping_patience:\n",
    "                    print(\"early stopping...\")\n",
    "                    early_stopping = True\n",
    "                    break\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e0352650b40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_nn(architecture=[500, 200, num_labels],\n\u001b[0m\u001b[0;32m      2\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          \u001b[0mearly_stopping_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m          optimizer_name=\"sgd\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_nn' is not defined"
     ]
    }
   ],
   "source": [
    "train_nn(architecture=[500, 200, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nn(architecture=[1024, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nn(architecture=[1024, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"sgd_decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nn(architecture=[1024, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"rmsprop\",\n",
    "         learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nn(architecture=[1024, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"adam\",\n",
    "         learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_nn(architecture=[1024, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20, \n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"rmsprop\",\n",
    "         learning_rate=0.01,\n",
    "         dropout=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch : 0\n",
      "Minibatch loss at epoch 0 and step 0: 494.704498\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at epoch 0 and step 500: 31.139427\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at epoch 0 and step 1000: 33.425377\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at epoch 0 and step 1500: 24.140602\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.5%\n",
      "Epoch : 1\n",
      "Minibatch loss at epoch 1 and step 0: 13.163796\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at epoch 1 and step 500: 18.566914\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at epoch 1 and step 1000: 3.680881\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at epoch 1 and step 1500: 24.066389\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.3%\n",
      "Epoch : 2\n",
      "Minibatch loss at epoch 2 and step 0: 4.144438\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at epoch 2 and step 500: 8.513011\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at epoch 2 and step 1000: 4.608531\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at epoch 2 and step 1500: 2.803911\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.3%\n",
      "Epoch : 3\n",
      "Minibatch loss at epoch 3 and step 0: 2.257047\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at epoch 3 and step 500: 1.491660\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at epoch 3 and step 1000: 4.305891\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at epoch 3 and step 1500: 4.118693\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 84.8%\n",
      "Epoch : 4\n",
      "Minibatch loss at epoch 4 and step 0: 5.521076\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at epoch 4 and step 500: 3.246701\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at epoch 4 and step 1000: 1.968289\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at epoch 4 and step 1500: 1.261908\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.6%\n",
      "Epoch : 5\n",
      "Minibatch loss at epoch 5 and step 0: 1.632128\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at epoch 5 and step 500: 1.464065\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at epoch 5 and step 1000: 1.742203\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at epoch 5 and step 1500: 0.800456\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.9%\n",
      "Epoch : 6\n",
      "Minibatch loss at epoch 6 and step 0: 0.926421\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at epoch 6 and step 500: 1.773379\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at epoch 6 and step 1000: 1.697915\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at epoch 6 and step 1500: 0.723955\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Epoch : 7\n",
      "Minibatch loss at epoch 7 and step 0: 1.203966\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at epoch 7 and step 500: 0.402014\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at epoch 7 and step 1000: 1.746082\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at epoch 7 and step 1500: 1.442610\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.0%\n",
      "Epoch : 8\n",
      "Minibatch loss at epoch 8 and step 0: 1.245024\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at epoch 8 and step 500: 0.653807\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at epoch 8 and step 1000: 1.806819\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at epoch 8 and step 1500: 0.820445\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Epoch : 9\n",
      "Minibatch loss at epoch 9 and step 0: 1.690301\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 9 and step 500: 0.932352\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at epoch 9 and step 1000: 2.539573\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at epoch 9 and step 1500: 1.566644\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.5%\n",
      "Epoch : 10\n",
      "Minibatch loss at epoch 10 and step 0: 1.375685\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at epoch 10 and step 500: 0.891621\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at epoch 10 and step 1000: 0.565610\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at epoch 10 and step 1500: 0.821953\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.0%\n",
      "Epoch : 11\n",
      "Minibatch loss at epoch 11 and step 0: 0.772510\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at epoch 11 and step 500: 0.731001\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 11 and step 1000: 0.909752\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at epoch 11 and step 1500: 1.525423\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.1%\n",
      "Epoch : 12\n",
      "Minibatch loss at epoch 12 and step 0: 2.368240\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at epoch 12 and step 500: 0.604242\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at epoch 12 and step 1000: 1.676209\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 12 and step 1500: 0.654934\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.0%\n",
      "Epoch : 13\n",
      "Minibatch loss at epoch 13 and step 0: 0.409131\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at epoch 13 and step 500: 0.853076\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 13 and step 1000: 2.562314\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at epoch 13 and step 1500: 1.222062\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 85.4%\n",
      "Epoch : 14\n",
      "Minibatch loss at epoch 14 and step 0: 1.304786\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 14 and step 500: 0.246739\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at epoch 14 and step 1000: 0.639189\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at epoch 14 and step 1500: 0.680214\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.3%\n",
      "Epoch : 15\n",
      "Minibatch loss at epoch 15 and step 0: 0.996844\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at epoch 15 and step 500: 2.220296\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at epoch 15 and step 1000: 0.573934\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at epoch 15 and step 1500: 1.107354\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Epoch : 16\n",
      "Minibatch loss at epoch 16 and step 0: 0.642003\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at epoch 16 and step 500: 1.591166\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at epoch 16 and step 1000: 1.238733\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 16 and step 1500: 2.656182\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.5%\n",
      "Epoch : 17\n",
      "Minibatch loss at epoch 17 and step 0: 3.755675\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at epoch 17 and step 500: 1.508085\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at epoch 17 and step 1000: 0.323967\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at epoch 17 and step 1500: 2.940991\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.1%\n",
      "Epoch : 18\n",
      "Minibatch loss at epoch 18 and step 0: 0.104431\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at epoch 18 and step 500: 0.423586\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at epoch 18 and step 1000: 0.947741\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at epoch 18 and step 1500: 0.384327\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.6%\n",
      "Epoch : 19\n",
      "Minibatch loss at epoch 19 and step 0: 0.920662\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at epoch 19 and step 500: 0.660909\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 19 and step 1000: 2.077642\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at epoch 19 and step 1500: 0.991121\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 92.9%\n"
     ]
    }
   ],
   "source": [
    "train_nn(architecture=[2048, num_labels],\n",
    "         batch_size=128, \n",
    "         num_epochs=20,\n",
    "         learning_rate=0.01,\n",
    "         early_stopping_patience=16,\n",
    "         optimizer_name=\"rmsprop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
